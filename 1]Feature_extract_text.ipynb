{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1]Feature_extract_text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPwyq1Ge9tcVGc1KoIsyLJ8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHxBB5jOAhby",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "db813034-fd03-4892-8688-939fcbe4120f"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus=['UNC played Duke in Basketball','Duke lost the Basketball game','I ate a sandwich']\n",
        "vectorizer=CountVectorizer(stop_words='english')\n",
        "print(vectorizer.fit_transform(corpus).todense())\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2 2 1 1 1 1 1]]\n",
            "{'unc': 7, 'played': 5, 'duke': 2, 'basketball': 1, 'lost': 4, 'game': 3, 'ate': 0, 'sandwich': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR-gHeP8CQ9-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "091f0e98-b67a-4061-83b6-3e0a0d20fd08"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus=['He ate the sandwiches','Every sandwich was eaten by him']\n",
        "vectorizer=CountVectorizer(binary=True,stop_words='english')\n",
        "print(vectorizer.fit_transform(corpus).todense())\n",
        "print(vectorizer.vocabulary_)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 1 1]]\n",
            "{'ate': 0, 'sandwiches': 3, 'sandwich': 2, 'eaten': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly_O47GXDlFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvYEH-DyGSc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "corpo=['I am gathering ingredients for the sandwich','There were many wizards at the gathering']\n",
        "lematizer=WordNetLemmatizer()\n",
        "print(lematizer.lemmatize('gathering','v'))\n",
        "print(lematizer.lemmatize('gathering','n'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nx6z4wUGjgG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "4940c694-e1a9-45a1-b90c-d42e52abbe30"
      },
      "source": [
        "#tokenizing\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "wordnet_tags=['n','v']\n",
        "stemmer=PorterStemmer()\n",
        "corpus=['He ate the sandwiches','Every sandwich was eaten by him']\n",
        "#Initially, we stem the documents in the corpus\n",
        "print(\"Stemmed:\",[[stemmer.stem(token) for token in word_tokenize(document)] for document in corpus])\n",
        "#POS tagging\n",
        "tagged_corpus=[pos_tag(word_tokenize(document)) for document in corpus]\n",
        "#function for lemmatizing the postagged words\n",
        "def lemmat(token,tag):\n",
        "  if tag[0].lower() in ['n','v']:\n",
        "    return lemmatizer.lemmatize(token,tag[0].lower())\n",
        "  return token\n",
        "#lemmatizing words\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "print(\"Pos-tagged words: \", tagged_corpus)\n",
        "print(\"Lemmatize: \",[[lemmat(token,tag) for token,tag in document] for document in tagged_corpus])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stemmed: [['He', 'ate', 'the', 'sandwich'], ['everi', 'sandwich', 'wa', 'eaten', 'by', 'him']]\n",
            "Pos-tagged words:  [[('He', 'PRP'), ('ate', 'VBD'), ('the', 'DT'), ('sandwiches', 'NNS')], [('Every', 'DT'), ('sandwich', 'NN'), ('was', 'VBD'), ('eaten', 'VBN'), ('by', 'IN'), ('him', 'PRP')]]\n",
            "Lemmatize:  [['He', 'eat', 'the', 'sandwich'], ['Every', 'sandwich', 'be', 'eat', 'by', 'him']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADa76aDz0C2i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f2ca8456-c6d1-4dcf-c53e-d92491381701"
      },
      "source": [
        "#instead of using a binary value for each element in feature vector, we will use an integer that denotes the number of times the word\n",
        "#has appeared in the document\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus=['The dog ate a sandwich, the wizard transfigured a sandwich,and I ate a sandwich']\n",
        "vectorizer=CountVectorizer(stop_words='english')\n",
        "print(vectorizer.fit_transform(corpus).todense())\n",
        "print(vectorizer.vocabulary_)\n",
        "#print(vectorizer.vocabulary_['dog'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 1 3 1 1]]\n",
            "{'dog': 1, 'ate': 0, 'sandwich': 2, 'wizard': 4, 'transfigured': 3}\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi-nMpUq72Sh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "5721fe0d-75af-4d41-edc7-0d59f3c53832"
      },
      "source": [
        "#Normalization, logarithmically scaled term frequencies and augmented term freq can represent the frequencies of terms in a document while mitigating\n",
        "#the effects of diff document sizes. However, another problem remains with these representations. The feature vector contains large weights for terms\n",
        "\n",
        "#A term's TF-IDf value is the product of its term frequency and inverse document freq. Can be implemented by class TfidVectorizer that wraps CountVectorizer\n",
        "#and TfidTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus=['The dog ate a sandwich and I ate a sandwich', 'the wizard transfigured a sandwich']\n",
        "vectorizer=TfidfVectorizer(stop_words='english')\n",
        "print(vectorizer.fit_transform(corpus).todense())\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.75458397 0.37729199 0.53689271 0.         0.        ]\n",
            " [0.         0.         0.44943642 0.6316672  0.6316672 ]]\n",
            "{'dog': 1, 'ate': 0, 'sandwich': 2, 'wizard': 4, 'transfigured': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbLTT7EH-SKp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "501c6911-1c0b-41c3-edb5-daf0828790d2"
      },
      "source": [
        "#hashvectorizer if tokens are not needed further for ML task because it maps tokens(words in document) to a column number in sparse matrix\n",
        "#and it impossible retrieve the token back\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "corpus=['the','ate','bacon','cat']\n",
        "vectorizer=HashingVectorizer(n_features=10) #select matrix size according the requirement, if n_features is small and corpus token are more than\n",
        "#columns, it can cause problem of matrix collision. Deafult value is 2^20\n",
        "print(vectorizer.fit_transform(corpus).todense())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.  0.  0.  0.  0.  0.  0.  0. -1.  0.]\n",
            " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0. -1.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}