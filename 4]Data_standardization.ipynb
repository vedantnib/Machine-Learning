{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4]Data_standardization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNt51xwSmxgjZG+4WH4Cs8w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedantnib/Machine-Learning/blob/master/4%5DData_standardization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlSlbrrOHZaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data Standardization\n",
        "#Many estimators perform better when they are trained on standardized data sets.\n",
        "#Standardized data has zero mean and unit variance."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlXxBAUjHZ_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Assume that a feature vector encode two explanatory variables. The first values of the first variable range from 0 to 1.\n",
        "#The second explanatory variable ranges from 0 to 100000. \n",
        "#The second feature must be scaled closer to {0,1} for the data to have unit variance. If a feature's variance is orders of magnitude greater than\n",
        "#the variances of other features, that feature may dominate the learning algorithm and prevent if from learning from other variables\n",
        "#The value of an explanatory variable can be standardized by subtracting the variable's mean and dividing by variable's standard deviation.\n",
        "#Data can be easily standardized using sklearn's 'scale' function:\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwhDH4kaGF2w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "fe9d0bac-693e-42c9-bf3e-53dbfb15427b"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "X=np.array([[0.,0.,5.,13.,9.,1.],\n",
        "           [0.,0.,13.,15.,10.,15.],\n",
        "           [0.,3.,15.,2.,0.,11.]])\n",
        "print(preprocessing.scale(X))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.         -0.70710678 -1.38873015  0.52489066  0.59299945 -1.35873244]\n",
            " [ 0.         -0.70710678  0.46291005  0.87481777  0.81537425  1.01904933]\n",
            " [ 0.          1.41421356  0.9258201  -1.39970842 -1.4083737   0.33968311]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}